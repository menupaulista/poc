# Web Scraper Modular - Doisporum.net

Este projeto foi refatorado seguindo princÃ­pios SOLID e arquitetura modular para melhor organizaÃ§Ã£o, manutenibilidade e testabilidade.

## ğŸ“ Estrutura Modular

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                 # Constantes e configuraÃ§Ãµes
â”œâ”€â”€ models/                   # Entidades de domÃ­nio
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ offer.py             # Modelo OfferItem
â”œâ”€â”€ protocols/               # Interfaces/Protocolos (Dependency Inversion)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ base.py             # HttpClient, Parser, Repository protocols
â”œâ”€â”€ clients/                 # ImplementaÃ§Ãµes de clientes HTTP
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ httpx_client.py     # Cliente HTTP assÃ­ncrono
â”œâ”€â”€ parsers/                # Parsers especÃ­ficos para diferentes pÃ¡ginas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ list_parser.py      # Parser para pÃ¡ginas de lista
â”‚   â””â”€â”€ detail_parser.py    # Parser para pÃ¡ginas de detalhes
â”œâ”€â”€ repositories/           # PersistÃªncia de dados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pandas_repository.py # RepositÃ³rio usando pandas
â””â”€â”€ services/               # ServiÃ§os de negÃ³cio
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ link_collector.py   # Coleta de links
    â”œâ”€â”€ detail_scraper.py   # Scraping de detalhes
    â””â”€â”€ coordinator.py      # Coordenador principal
```

## ğŸ—ï¸ Arquitetura

### PrincÃ­pios SOLID Aplicados

1. **Single Responsibility Principle (SRP)**: Cada classe tem uma Ãºnica responsabilidade
   - `OfferItem`: Representa uma oferta
   - `AsyncHttpxClient`: Gerencia requisiÃ§Ãµes HTTP
   - `DoisPorUmDetailParser`: Parse de pÃ¡ginas de detalhes
   - `LinkCollector`: Coleta links de pÃ¡ginas de lista

2. **Open/Closed Principle (OCP)**: ExtensÃ­vel sem modificar cÃ³digo existente
   - Novos parsers podem ser adicionados implementando os protocolos
   - Novos repositÃ³rios podem ser criados sem alterar o cÃ³digo existente

3. **Liskov Substitution Principle (LSP)**: ImplementaÃ§Ãµes podem ser substituÃ­das
   - Qualquer implementaÃ§Ã£o de `HttpClient` pode ser usada
   - Qualquer parser que implemente `DetailParser` Ã© intercambiÃ¡vel

4. **Interface Segregation Principle (ISP)**: Interfaces especÃ­ficas
   - `HttpClient` para operaÃ§Ãµes HTTP
   - `DetailParser` para parsing de detalhes
   - `ListPageParser` para parsing de listas
   - `OfferRepository` para persistÃªncia

5. **Dependency Inversion Principle (DIP)**: DependÃªncia de abstraÃ§Ãµes
   - ServiÃ§os dependem de protocolos, nÃ£o de implementaÃ§Ãµes concretas
   - InjeÃ§Ã£o de dependÃªncia no `main()`

### PadrÃµes de Design

- **Strategy Pattern**: Diferentes parsers para diferentes tipos de pÃ¡gina
- **Repository Pattern**: AbstraÃ§Ã£o da camada de persistÃªncia
- **Service Layer**: LÃ³gica de negÃ³cio separada em serviÃ§os
- **Coordinator Pattern**: OrquestraÃ§Ã£o do processo completo

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

```bash
# Criar ambiente virtual
uv venv
source .venv/bin/activate  # Windows: .venv\\Scripts\\activate

# Instalar dependÃªncias
uv add "httpx[http2]==0.28.1" beautifulsoup4 lxml pandas tqdm
```

### ExecuÃ§Ã£o

```bash
# Usar o scraper original (legado)
python scraper.py --seed-url "https://doisporum.net/" --max-items 120

# Usar o scraper modular
python main_modular.py --seed-url "https://doisporum.net/" --max-items 120

# Teste rÃ¡pido
python test_modular.py
```

### ParÃ¢metros

- `--seed-url`: URL inicial para scraping
- `--max-items`: NÃºmero mÃ¡ximo de itens (padrÃ£o: 120)
- `--rate-limit-seconds`: Tempo entre requisiÃ§Ãµes (padrÃ£o: 0.8s)
- `--max-concurrency`: RequisiÃ§Ãµes simultÃ¢neas (padrÃ£o: 6)
- `--csv-path`: Caminho do arquivo CSV de saÃ­da
- `--jsonl-path`: Caminho do arquivo JSONL de saÃ­da
- `--timeout`: Timeout das requisiÃ§Ãµes (padrÃ£o: 20s)
- `--user-agent`: User-Agent customizado

## ğŸ§ª Testes

```bash
# Teste bÃ¡sico da estrutura modular
python test_modular.py

# Teste do scraper original (para comparaÃ§Ã£o)
python test_world_wine.py
```

## ğŸ“¦ MÃ³dulos Detalhados

### models/offer.py
- `OfferItem`: Dataclass que representa uma oferta com todos os campos necessÃ¡rios

### protocols/base.py
- `HttpClient`: Interface para clientes HTTP
- `DetailParser`: Interface para parsers de detalhes
- `ListPageParser`: Interface para parsers de listas
- `OfferRepository`: Interface para repositÃ³rios

### clients/httpx_client.py
- `AsyncHttpxClient`: Cliente HTTP assÃ­ncrono com rate limiting, retry logic e HTTP/2

### parsers/
- `DoisPorUmListPageParser`: Extrai links de detalhes e paginaÃ§Ã£o
- `DoisPorUmDetailParser`: Extrai dados completos das pÃ¡ginas de oferta

### repositories/pandas_repository.py
- `PandasOfferRepository`: Salva dados em CSV e JSONL usando pandas

### services/
- `LinkCollector`: Coleta links usando BFS com paginaÃ§Ã£o
- `DetailScraper`: Faz scraping de detalhes com controle de concorrÃªncia
- `ScrapeCoordinator`: Orquestra todo o processo

## ğŸ”§ Extensibilidade

### Adicionando um Novo Parser

```python
# src/parsers/new_parser.py
from src.protocols.base import DetailParser
from src.models.offer import OfferItem

class NewDetailParser:
    def parse(self, html: str, url: str) -> Optional[OfferItem]:
        # Implementar lÃ³gica especÃ­fica
        return OfferItem(...)

# Uso no main.py
detail_parser = NewDetailParser()
```

### Adicionando um Novo RepositÃ³rio

```python
# src/repositories/json_repository.py
from src.protocols.base import OfferRepository

class JsonOfferRepository:
    def save_csv(self, items: List[OfferItem], path: str) -> None:
        # ImplementaÃ§Ã£o especÃ­fica
        pass
    
    def save_jsonl(self, items: List[OfferItem], path: str) -> None:
        # ImplementaÃ§Ã£o especÃ­fica
        pass
```

## ğŸ” Vantagens da RefatoraÃ§Ã£o

1. **Testabilidade**: Cada componente pode ser testado isoladamente
2. **Manutenibilidade**: MudanÃ§as em um mÃ³dulo nÃ£o afetam outros
3. **ReutilizaÃ§Ã£o**: Componentes podem ser reutilizados em outros projetos
4. **Flexibilidade**: FÃ¡cil trocar implementaÃ§Ãµes (ex: httpx por requests)
5. **Legibilidade**: CÃ³digo mais organizado e fÃ¡cil de entender
6. **Escalabilidade**: Estrutura preparada para crescimento

## ğŸš§ MigraÃ§Ã£o

O arquivo `scraper.py` original foi mantido para compatibilidade. Para migrar:

1. Use `main_modular.py` em vez de `scraper.py`
2. Teste com `test_modular.py`
3. ApÃ³s validaÃ§Ã£o, remova `scraper.py` se necessÃ¡rio

A API e parÃ¢metros de linha de comando sÃ£o idÃªnticos, garantindo compatibilidade total.
